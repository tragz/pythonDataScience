{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Continuous Batching\n",
    "\n",
    "- The key idea behind continuous batching constantly swap out requests from the batch that have completed generation for requests in the queue that are waiting to be processed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad80cdb720948fda"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:49:19.884062Z",
     "start_time": "2025-02-25T08:49:19.881186Z"
    }
   },
   "id": "df3ff2174853f872",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:49:37.021955Z",
     "start_time": "2025-02-25T08:49:35.795078Z"
    }
   },
   "id": "335f9d3a7a61301",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:49:48.405059Z",
     "start_time": "2025-02-25T08:49:48.394342Z"
    }
   },
   "id": "57664d7abe833585",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# multiple prompts of varying lengths to send to the model at once\n",
    "prompts = [\n",
    "    \"The quick brown fox jumped over the\",\n",
    "    \"The rain in Spain falls\",\n",
    "    \"What comes up must\",\n",
    "]\n",
    "\n",
    "# note: padding=True ensures the padding token will be inserted into the tokenized tensors\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:49:59.465103Z",
     "start_time": "2025-02-25T08:49:59.460272Z"
    }
   },
   "id": "c6b3986aab867ff8",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define needed functions for batching\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2a5c57c0fe174c6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids, outputs.past_key_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:50:28.329238Z",
     "start_time": "2025-02-25T08:50:28.323574Z"
    }
   },
   "id": "be8b08e8b675f668",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_batch(inputs, max_tokens):\n",
    "    # create a list of tokens for every input in the batch\n",
    "    generated_tokens = [[] for _ in range(inputs[\"input_ids\"].shape[0])]\n",
    "    \n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "    \n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "    }\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_ids, past_key_values = generate_batch_tokens_with_past(next_inputs)\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_ids.reshape((-1, 1)),  # '-1' here means the remaining elements for this dim\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1,  # increment last, discard the rest\n",
    "            \"attention_mask\": torch.cat([\n",
    "                next_inputs[\"attention_mask\"],\n",
    "                torch.ones((next_token_ids.shape[0], 1)),  # concatenate vector of 1's with shape [batch_size]\n",
    "            ], dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "\n",
    "        next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            generated_tokens[i].append(token)\n",
    "    return [\"\".join(tokens) for tokens in generated_tokens]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:50:43.018554Z",
     "start_time": "2025-02-25T08:50:43.015229Z"
    }
   },
   "id": "e3bd647ac646c8b8",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the requests to be processed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "863eff8d05e21518"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# seed the random number generator so our results are deterministic\n",
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 32\n",
    "batch_size = 8\n",
    "\n",
    "# requests waiting to be processed\n",
    "# requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[0], 100 if i % batch_size == 0 else 10)\n",
    "    for i in range(queue_size)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:51:48.210280Z",
     "start_time": "2025-02-25T08:51:48.204093Z"
    }
   },
   "id": "6bfed24ed2308102",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[('The quick brown fox jumped over the', 100),\n ('The quick brown fox jumped over the', 10),\n ('The quick brown fox jumped over the', 10),\n ('The quick brown fox jumped over the', 10),\n ('The quick brown fox jumped over the', 10),\n ('The quick brown fox jumped over the', 10),\n ('The quick brown fox jumped over the', 10),\n ('The quick brown fox jumped over the', 10)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_queue[:8]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:52:54.206564Z",
     "start_time": "2025-02-25T08:52:54.200205Z"
    }
   },
   "id": "a00910c460b9356a",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batches = [\n",
    "    request_queue[i:i + batch_size]\n",
    "    for i in range(0, len(request_queue), batch_size)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:53:17.296130Z",
     "start_time": "2025-02-25T08:53:17.290296Z"
    }
   },
   "id": "5ea7973584dd9db8",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:53:29.388923Z",
     "start_time": "2025-02-25T08:53:29.381648Z"
    }
   },
   "id": "4d45b33c90374d9d",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[('The quick brown fox jumped over the', 100),\n ('The quick brown fox jumped over the', 10),\n ('The quick brown fox jumped over the', 10),\n ('The quick brown fox jumped over the', 10),\n ('The quick brown fox jumped over the', 10),\n ('The quick brown fox jumped over the', 10),\n ('The quick brown fox jumped over the', 10),\n ('The quick brown fox jumped over the', 10)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:55:45.024439Z",
     "start_time": "2025-02-25T08:55:45.020220Z"
    }
   },
   "id": "fc3846df3c12ee65",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Processing batches \n",
    "\n",
    "**Note:** Your results might differ somewhat from those shown in the video, but they will still follow the same pattern as explained by the instructor."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23cc6a3e4dd315b6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8: 100%|██████████| 4/4 [00:14<00:00,  3.63s/it, max_tokens=100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration 14.509596109390259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate tokens for all batches and record duration\n",
    "t0 = time.time()\n",
    "with tqdm(total=len(batches), desc=f\"bs={batch_size}\") as pbar:\n",
    "    for i, batch in enumerate(batches):\n",
    "        # to accommodate all the requests with our \n",
    "        # current implementation, we take the max of\n",
    "        # all the tokens to generate among the requests\n",
    "        batch_max_tokens = [b[1] for b in batch]\n",
    "        max_tokens = max(batch_max_tokens)\n",
    "        pbar.set_postfix({'max_tokens': max_tokens})\n",
    "        \n",
    "        batch_prompts = [b[0] for b in batch]\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, padding=True, return_tensors=\"pt\")\n",
    "        generate_batch(inputs, max_tokens=max_tokens)\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "duration_s = time.time() - t0\n",
    "print(\"duration\", duration_s)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T08:57:42.830184Z",
     "start_time": "2025-02-25T08:57:28.320394Z"
    }
   },
   "id": "1cff9d2f317224fa",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's try continuous batching\n",
    "\n",
    "- This time, rather than processing each batch to completion, you will use continuous batching to dynamically swap in and out inputs from the queue.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e826a9f566c2f719"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'filter_batch' from 'helpers' (/Users/raghav.tanaji/Desktop/gitrepos/LEARNING/pythonDataScience/DeepLearning-AI/EfficientlyServingLLMs/helpers.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mhelpers\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mhelpers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m init_batch, generate_next_token\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mhelpers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m merge_batches, filter_batch\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'filter_batch' from 'helpers' (/Users/raghav.tanaji/Desktop/gitrepos/LEARNING/pythonDataScience/DeepLearning-AI/EfficientlyServingLLMs/helpers.py)"
     ]
    }
   ],
   "source": [
    "import helpers\n",
    "from helpers import init_batch, generate_next_token\n",
    "from helpers import merge_batches, filter_batch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:04:50.246975Z",
     "start_time": "2025-02-25T09:04:50.238466Z"
    }
   },
   "id": "e8c254ca773367bd",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8:   0%|          | 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'filter_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 47\u001B[0m\n\u001B[1;32m     44\u001B[0m         cached_batch \u001B[38;5;241m=\u001B[39m generate_next_token(cached_batch)\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;66;03m# remove any inputs that have finished generation\u001B[39;00m\n\u001B[0;32m---> 47\u001B[0m         cached_batch, removed_indices \u001B[38;5;241m=\u001B[39m \u001B[43mfilter_batch\u001B[49m(cached_batch)\n\u001B[1;32m     48\u001B[0m         pbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mlen\u001B[39m(removed_indices))\n\u001B[1;32m     50\u001B[0m duration_s \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n",
      "\u001B[0;31mNameError\u001B[0m: name 'filter_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# seed the random number generator so our results are deterministic\n",
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 32\n",
    "batch_size = 8\n",
    "\n",
    "# requests waiting to be processed\n",
    "# this time requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[0], 100 if i % batch_size == 0 else 10)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "\n",
    "t0 = time.time()\n",
    "with tqdm(total=len(request_queue), desc=f\"bs={batch_size}\") as pbar:\n",
    "    # first, let's seed the initial cached_batch\n",
    "    # with the first `batch_size` inputs\n",
    "    # and run the initial prefill step\n",
    "    batch = init_batch(request_queue[:batch_size])\n",
    "    cached_batch = generate_next_token(batch)\n",
    "    request_queue = request_queue[batch_size:]\n",
    "\n",
    "    # continue until both the request queue is \n",
    "    # fully drained and every input\n",
    "    # within the cached_batch has completed generation\n",
    "    while (\n",
    "        len(request_queue) > 0 or\n",
    "        cached_batch[\"input_ids\"].size(0) > 0\n",
    "    ):\n",
    "        batch_capacity = (\n",
    "            batch_size - cached_batch[\"input_ids\"].size(0)\n",
    "        )\n",
    "        if batch_capacity > 0 and len(request_queue) > 0:\n",
    "            # prefill\n",
    "            new_batch = init_batch(request_queue[:batch_capacity])\n",
    "            new_batch = generate_next_token(new_batch)\n",
    "            request_queue = request_queue[batch_capacity:]\n",
    "\n",
    "            # merge\n",
    "            cached_batch = merge_batches(cached_batch, new_batch)\n",
    "\n",
    "        # decode\n",
    "        cached_batch = generate_next_token(cached_batch)\n",
    "\n",
    "        # remove any inputs that have finished generation\n",
    "        cached_batch, removed_indices = filter_batch(cached_batch)\n",
    "        pbar.update(len(removed_indices))\n",
    "\n",
    "duration_s = time.time() - t0\n",
    "print(\"duration\", duration_s)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:04:50.955455Z",
     "start_time": "2025-02-25T09:04:50.838830Z"
    }
   },
   "id": "d867302b908913fc",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "441fc9db86c103b8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
