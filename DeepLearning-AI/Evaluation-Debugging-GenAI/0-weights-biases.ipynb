{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to W&B\n",
    "\n",
    "We will add wandb to sprite classification model training, so that we can track and visualize important metrics, gain insights into our model's behavior and make informed decisions for model improvements. We will also see how to compare and analyze different experiments, collaborate with team members, and reproduce results effectively.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ccb187352446301"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from utilities import get_dataloaders\n",
    "import wandb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-18T12:45:07.286558Z",
     "start_time": "2024-07-18T12:45:07.281690Z"
    }
   },
   "id": "b7772f78ea9fcf99",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sprite classification\n",
    "\n",
    "We will build a simple model to classify sprites. You can see some examples of sprites and corresponding classes in the image below.\n",
    "\n",
    "<img src=\"sprite_sample.png\" alt=\"Alt Text\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da62128dce0ec438"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "INPUT_SIZE = 3 * 16 * 16\n",
    "OUTPUT_SIZE = 5\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_WORKERS = 2\n",
    "CLASSES = [\"hero\", \"non-hero\", \"food\", \"spell\", \"side-facing\"]\n",
    "DATA_DIR = Path('./data/')\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# simple classifier model with 2 linear layers\n",
    "def get_model(dropout):\n",
    "    \"Simple MLP with Dropout\"\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(INPUT_SIZE, HIDDEN_SIZE),\n",
    "        nn.BatchNorm1d(HIDDEN_SIZE),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "    ).to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-18T12:45:08.803984Z",
     "start_time": "2024-07-18T12:45:08.798852Z"
    }
   },
   "id": "6ec5623c78e97415",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Let's define a config object to store our hyperparameters\n",
    "# SimpleNamespace - provides a mechanism to instantiate an object that can hold attributes and nothing else. empty class with fancier __init__() and helpful __repr__()\n",
    "config = SimpleNamespace(\n",
    "    epochs = 2,\n",
    "    batch_size = 128,\n",
    "    lr = 1e-5,\n",
    "    dropout = 0.5,\n",
    "    slice_size = 10_000,\n",
    "    valid_pct = 0.2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-18T12:45:09.532461Z",
     "start_time": "2024-07-18T12:45:09.529732Z"
    }
   },
   "id": "55b82b32d69f43b8",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "namespace(epochs=2,\n          batch_size=128,\n          lr=1e-05,\n          dropout=0.5,\n          slice_size=10000,\n          valid_pct=0.2)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-18T12:45:10.348709Z",
     "start_time": "2024-07-18T12:45:10.343821Z"
    }
   },
   "id": "963dca17c4e32c61",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \"Train a model with a given config\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"dlai_intro\",\n",
    "        config = config\n",
    "    )\n",
    "    \n",
    "    # Get the data\n",
    "    train_dl, valid_dl = get_dataloaders(\n",
    "        DATA_DIR, config.batch_size, config.slice_size, config.valid_pct\n",
    "    )\n",
    "    n_steps_per_epoch = math.ceil(len(train_dl.dataset)/config.batch_size)\n",
    "    \n",
    "    # A simple MLP Model\n",
    "    model = get_model(config.dropout)\n",
    "    \n",
    "    # Make the loss and optimizer\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=config.lr)\n",
    "    \n",
    "    example_ct = 0\n",
    "    for epoch in tqdm(range(config.epochs), total=config.epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for step, (images, labels) in enumerate(train_dl):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            train_loss = loss_func(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            example_ct += len(images)\n",
    "            metrics = {\n",
    "                \"train/train_loss\" : train_loss,\n",
    "                \"train/epoch\" : epoch + 1,\n",
    "                \"train/example_ct\" : example_ct\n",
    "            }\n",
    "            wandb.log(metrics)\n",
    "            \n",
    "        # Compute validation metrics, log images on last epoch\n",
    "        val_loss, accuracy = validate_model(model, valid_dl, loss_func)\n",
    "        # Compute train and validation metrics\n",
    "        val_metrics = {\n",
    "            \"val/val_loss\": val_loss,\n",
    "            \"val/val_accuracy\": accuracy\n",
    "        }\n",
    "        wandb.log(val_metrics)\n",
    "    \n",
    "    wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-18T12:45:10.971423Z",
     "start_time": "2024-07-18T12:45:10.968459Z"
    }
   },
   "id": "a8c9a3e94b85de8e",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def validate_model(model, valid_dl, loss_func):\n",
    "    \"Compute the performance of the model on the validation dataset\"\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.inference_mode:\n",
    "        for i, (images, labels) in enumerate(valid_dl):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            val_loss += loss_func(outputs, labels) * labels.size(0)\n",
    "            \n",
    "            # compute accuracy and accumulate\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)      "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-18T12:49:05.529630Z",
     "start_time": "2024-07-18T12:49:05.525658Z"
    }
   },
   "id": "c780d4cea196ade8",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "# W&B account\n",
    "\n",
    "The next cell will log you into the Weights and Biases site anonymously, that is without a unique login. You can also sign up for a free account if you wish to save your work, but that is not needed to finish the course."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c77e553b46aa68f9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: (1) Private W&B dashboard, no account required\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: (2) Use an existing W&B account\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Invalid choice\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: You chose 'Use an existing W&B account'\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /Users/raghav.tanaji/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(anonymous=\"allow\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-18T12:52:21.600756Z",
     "start_time": "2024-07-18T12:49:40.113119Z"
    }
   },
   "id": "fb921c51872fb1c0",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mraghav30\u001B[0m (\u001B[33mraghav-tanaji\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.17.4"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/raghav.tanaji/Desktop/gitrepos/pythonDataScience/DeepLearning-AI/Evaluation-Debugging-GenAI/wandb/run-20240718_182229-vc1xq8t5</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/raghav-tanaji/dlai_intro/runs/vc1xq8t5' target=\"_blank\">wandering-donkey-1</a></strong> to <a href='https://wandb.ai/raghav-tanaji/dlai_intro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/raghav-tanaji/dlai_intro' target=\"_blank\">https://wandb.ai/raghav-tanaji/dlai_intro</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/raghav-tanaji/dlai_intro/runs/vc1xq8t5' target=\"_blank\">https://wandb.ai/raghav-tanaji/dlai_intro/runs/vc1xq8t5</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/sprites_1788_16x16.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[14], line 10\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(config)\u001B[0m\n\u001B[1;32m      4\u001B[0m wandb\u001B[38;5;241m.\u001B[39minit(\n\u001B[1;32m      5\u001B[0m     project\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdlai_intro\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      6\u001B[0m     config \u001B[38;5;241m=\u001B[39m config\n\u001B[1;32m      7\u001B[0m )\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Get the data\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m train_dl, valid_dl \u001B[38;5;241m=\u001B[39m \u001B[43mget_dataloaders\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mDATA_DIR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mslice_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalid_pct\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m n_steps_per_epoch \u001B[38;5;241m=\u001B[39m math\u001B[38;5;241m.\u001B[39mceil(\u001B[38;5;28mlen\u001B[39m(train_dl\u001B[38;5;241m.\u001B[39mdataset)\u001B[38;5;241m/\u001B[39mconfig\u001B[38;5;241m.\u001B[39mbatch_size)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# A simple MLP Model\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/gitrepos/pythonDataScience/DeepLearning-AI/Evaluation-Debugging-GenAI/utilities.py:389\u001B[0m, in \u001B[0;36mget_dataloaders\u001B[0;34m(data_dir, batch_size, slice_size, valid_pct)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_dataloaders\u001B[39m(data_dir, batch_size, slice_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, valid_pct\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGet train/val dataloaders for classification on sprites dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 389\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43mCustomDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_np\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margmax\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    390\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m slice_size:\n\u001B[1;32m    391\u001B[0m         dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39msubset(slice_size)\n",
      "File \u001B[0;32m~/Desktop/gitrepos/pythonDataScience/DeepLearning-AI/Evaluation-Debugging-GenAI/utilities.py:355\u001B[0m, in \u001B[0;36mCustomDataset.from_np\u001B[0;34m(cls, path, sfilename, lfilename, transform, null_context, argmax)\u001B[0m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_np\u001B[39m(\u001B[38;5;28mcls\u001B[39m, \n\u001B[1;32m    353\u001B[0m             path, \n\u001B[1;32m    354\u001B[0m             sfilename\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msprites_1788_16x16.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m, lfilename\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msprite_labels_nc_1788_16x16.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m, transform\u001B[38;5;241m=\u001B[39mdefault_tfms, null_context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, argmax\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 355\u001B[0m     sprites \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43msfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    356\u001B[0m     slabels \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(Path(path)\u001B[38;5;241m/\u001B[39mlfilename)\n\u001B[1;32m    357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(sprites, slabels, transform, null_context, argmax)\n",
      "File \u001B[0;32m~/Desktop/gitrepos/pythonDataScience/venv/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001B[0m, in \u001B[0;36mload\u001B[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[1;32m    425\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 427\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos_fspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    428\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/sprites_1788_16x16.npy'"
     ]
    }
   ],
   "source": [
    "train_model(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-18T12:52:33.255734Z",
     "start_time": "2024-07-18T12:52:29.848465Z"
    }
   },
   "id": "4259f272fcd84a69",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_model(config)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd5f8c64db2f5367"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
